# -*- coding: utf-8 -*-
"""Makhdoom_Faiqueali_Assignement_part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13P2egvB0gsQ5FOCGpdCnqJdHEBRDb0Xn
"""

import nltk
import pandas as pd
import numpy as np
import re

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('averaged_perceptron_tagger_eng', quiet=True)

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords

# Load the novel
with open('/content/moby.txt', 'r') as f:
    moby_raw = f.read()

# Create NLTK Text object
moby_tokens = nltk.word_tokenize(moby_raw)
text1 = nltk.Text(moby_tokens)

print(f"Loaded Moby Dick")
print(f"Raw text length: {len(moby_raw)} characters")
print(f"First 200 characters: {moby_raw[:200]}")

import nltk

# Example text (replace with your actual text)
moby_raw = "Call me Ishmael. Some years ago—never mind how long precisely..."

def question_one():
    """
    Calculate the lexical diversity of the text.

    Returns:
        float: Ratio of unique tokens to total tokens
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Count unique tokens
    unique_tokens = set(tokens)

    # Lexical diversity = unique / total
    lexical_diversity = len(unique_tokens) / len(tokens)

    return lexical_diversity

# Test the function
q1_result = question_one()
print(f"Lexical diversity: {q1_result:.2f}")

import nltk

# Example text (replace with your actual text)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a Whale. The whale was huge."

def question_two():
    """
    Calculate the percentage of tokens that are 'whale' or 'Whale'.

    Returns:
        float: Percentage of whale tokens
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Count tokens that are 'whale' or 'Whale'
    whale_count = sum(1 for t in tokens if t.lower() == 'whale')

    # Calculate percentage
    percentage = (whale_count / len(tokens)) * 50

    return percentage

# Test the function
q2_result = question_two()
print(f"Percentage of 'whale'/'Whale': {q2_result:.2f}%")

import nltk
from nltk import FreqDist

# Example text (replace with your actual text)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale. The whale was huge. Whale, whale, whale!"

def question_three():
    """
    Find the 20 most frequent tokens and their frequencies.

    Returns:
        list: List of 20 tuples (token, frequency) sorted by frequency descending
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Create frequency distribution
    freq_dist = FreqDist(tokens)

    # Return the 20 most common tokens as (token, frequency)
    return freq_dist.most_common(20)

# Test the function
q3_result = question_three()
print("20 most frequent tokens:")
for token, freq in q3_result:
    print(f"  {token}: {freq}")

import nltk
from nltk import FreqDist

# Example text: replace with your actual text (e.g., moby_raw)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale. " \
           "The whale was huge. " * 200  # repeating to simulate frequency > 150

def question_four():
    """
    Find tokens with length > 5 and frequency > 150.

    Returns:
        list: Alphabetically sorted list of tokens
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Frequency distribution
    freq_dist = FreqDist(tokens)

    # Filter tokens: length > 5 and frequency > 150
    filtered_tokens = [t for t, f in freq_dist.items() if len(t) > 5 and f > 150]

    # Sort alphabetically
    filtered_tokens.sort()

    return filtered_tokens

# Test the function
q4_result = question_four()
print(f"Found {len(q4_result)} tokens:")
print(q4_result)

import nltk

# Example text (replace with your full text, e.g., moby_raw)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale."

def question_five():
    """
    Find the longest word in the text.

    Returns:
        tuple: (longest_word, length)
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Remove punctuation (optional)
    words = [t for t in tokens if t.isalpha()]

    # Find the longest word
    if words:
        longest_word = max(words, key=len)
        return (longest_word, len(longest_word))
    else:
        return (None, 0)

# Test the function
q5_result = question_five()
print(f"Longest word: '{q5_result[0]}' with length {q5_result[1]}")

from nltk.parse.chart import TopDownInitRule
import nltk
from nltk import FreqDist

# Example text (replace with your full corpus, e.g., moby_raw)
# Simulated text for demonstration
moby_raw = ("Call me Ishmael. Some years ago I thought I saw a whale. " * 3000)  # repeated to simulate high frequency

def question_six():
    """
    Find words with frequency > 2000.

    Returns:
        list: List of tuples (frequency, word) sorted by frequency descending
    """
    # Tokenize the text
    tokens = nltk.word_tokenize(moby_raw)

    # Create frequency distribution
    freq_dist = FreqDist(tokens)

    # Filter words with frequency > 2000
    filtered = [(f, w) for w, f in freq_dist.items() if f > 2000]

    # Sort by frequency descending
    filtered.sort(reverse=True)

    return filtered

# Test the function
q6_result = question_six()
print("Words with frequency > 2000:")
for freq, word in q6_result:
    print(f"  {word}: {freq}")

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

# Example text (replace with your full text, e.g., moby_raw)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale. The whale was huge!"

# Ensure required NLTK data is downloaded
nltk.download('punkt')

def question_seven():
    """
    Calculate the average number of tokens per sentence.

    Returns:
        float: Average tokens per sentence
    """
    # Step 1: Split text into sentences
    sentences = sent_tokenize(moby_raw)

    # Step 2: Count tokens in each sentence
    token_counts = [len(word_tokenize(sent)) for sent in sentences]

    # Step 3: Calculate average
    avg_tokens = sum(token_counts) / len(token_counts) if sentences else 0

    return avg_tokens

# Test the function
q7_result = question_seven()
print(f"Average tokens per sentence: {q7_result:.2f}")

import nltk
from nltk.corpus import stopwords
from nltk import FreqDist
from nltk.tokenize import word_tokenize
import string

# Example text (replace with your full text, e.g., moby_raw)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale. The whale was huge. Whale, whale, whale!"

# Ensure required NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')

def question_eight():
    """
    Find 10 most common words after removing stop words.

    Returns:
        list: List of 10 tuples (word, frequency) sorted by frequency descending
    """
    # Step 1: Tokenize and lowercase
    tokens = [t.lower() for t in word_tokenize(moby_raw)]

    # Step 2: Remove punctuation
    tokens = [t for t in tokens if t.isalpha()]

    # Step 3: Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [t for t in tokens if t not in stop_words]

    # Step 4: Frequency distribution
    freq_dist = FreqDist(filtered_tokens)

    # Step 5: Return 10 most common
    return freq_dist.most_common(10)

# Test the function
q8_result = question_eight()
print("10 most common words (excluding stop words):")
for word, freq in q8_result:
    print(f"  {word}: {freq}")

import nltk
from nltk.stem import PorterStemmer
from nltk import FreqDist
from nltk.tokenize import word_tokenize

# Example text (replace with your full text, e.g., moby_raw)
moby_raw = "Call me Ishmael. Some years ago, I thought I saw a whale. The whale was huge. Whale, whales, whale!"

# Ensure required NLTK data is downloaded
nltk.download('punkt')

def question_nine():
    """
    Find 10 most common stems using Porter stemmer.

    Returns:
        list: List of 10 tuples (stem, frequency) sorted by frequency descending
    """
    # Step 1: Tokenize and lowercase
    tokens = [t.lower() for t in word_tokenize(moby_raw)]

    # Step 2: Keep only alphabetic tokens
    words = [t for t in tokens if t.isalpha()]

    # Step 3: Apply Porter stemming
    stemmer = PorterStemmer()
    stems = [stemmer.stem(w) for w in words]

    # Step 4: Count stem frequencies
    freq_dist = FreqDist(stems)

    # Step 5: Return 10 most common stems
    return freq_dist.most_common(10)

# Test the function
q9_result = question_nine()
print("10 most common stems:")
for stem, freq in q9_result:
    print(f"  {stem}: {freq}")

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Example text (replace with your actual moby_raw)
moby_raw = ("Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, "
            "and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. "
            "It is a way I have of driving off the spleen and regulating the circulation.")

# Ensure required NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def question_ten():
    """
    Preprocess the first 1000 characters of Moby Dick.

    Returns:
        list: List of preprocessed tokens
    """
    text = moby_raw[:1000]
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    # Step 1: Tokenize
    tokens = word_tokenize(text)

    # Step 2: Lowercase
    tokens = [t.lower() for t in tokens]

    # Step 3: Keep only alphabetic tokens
    words = [t for t in tokens if t.isalpha()]

    # Step 4: Remove stop words
    words = [t for t in words if t not in stop_words]

    # Step 5: Lemmatize
    preprocessed = [lemmatizer.lemmatize(t) for t in words]

    return preprocessed

# Test the function
q10_result = question_ten()
print(f"Number of preprocessed tokens: {len(q10_result)}")
print(f"First 20 tokens: {q10_result[:20]}")

# Run this cell to verify all functions exist and return correct types
print("Checking functions...")

try:
    r1 = question_one()
    assert isinstance(r1, float), "question_one should return a float"
    print("✓ question_one: OK")
except Exception as e:
    print(f"✗ question_one: {e}")

try:
    r2 = question_two()
    assert isinstance(r2, float), "question_two should return a float"
    print("✓ question_two: OK")
except Exception as e:
    print(f"✗ question_two: {e}")

try:
    r3 = question_three()
    assert isinstance(r3, list) and len(r3) == 20, "question_three should return a list of 20 tuples"
    print("✓ question_three: OK")
except Exception as e:
    print(f"✗ question_three: {e}")

try:
    r4 = question_four()
    assert isinstance(r4, list), "question_four should return a list"
    print("✓ question_four: OK")
except Exception as e:
    print(f"✗ question_four: {e}")

try:
    r5 = question_five()
    assert isinstance(r5, tuple) and len(r5) == 2, "question_five should return a tuple of 2 elements"
    print("✓ question_five: OK")
except Exception as e:
    print(f"✗ question_five: {e}")

try:
    r6 = question_six()
    assert isinstance(r6, list), "question_six should return a list"
    print("✓ question_six: OK")
except Exception as e:
    print(f"✗ question_six: {e}")

try:
    r7 = question_seven()
    assert isinstance(r7, float), "question_seven should return a float"
    print("✓ question_seven: OK")
except Exception as e:
    print(f"✗ question_seven: {e}")

try:
    r8 = question_eight()
    assert isinstance(r8, list) and len(r8) == 10, "question_eight should return a list of 10 tuples"
    print("✓ question_eight: OK")
except Exception as e:
    print(f"✗ question_eight: {e}")

try:
    r9 = question_nine()
    assert isinstance(r9, list) and len(r9) == 10, "question_nine should return a list of 10 tuples"
    print("✓ question_nine: OK")
except Exception as e:
    print(f"✗ question_nine: {e}")

try:
    r10 = question_ten()
    assert isinstance(r10, list), "question_ten should return a list"
    print("✓ question_ten: OK")
except Exception as e:
    print(f"✗ question_ten: {e}")

print("\nDone! Export this notebook as .py file when all functions pass.")
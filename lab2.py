# -*- coding: utf-8 -*-
"""LAB2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ke_PgaKZhVVlQLbpOba1XCtcH4rbazii
"""

import nltk
import spacy
import re
import string

# Download required NLTK data
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

# Download and load spaCy model
nlp = spacy.load('en_core_web_sm')

print("Setup complete!")

# Simple tokenization using split()
text = "Natural Language Processing is fascinating!"

# Basic split on whitespace
tokens_basic = text.split()
print("Basic split():", tokens_basic)

# Problem: punctuation is attached to words!
print("Last token:", tokens_basic[-1])  # 'fascinating!' includes the !

from nltk.tokenize import word_tokenize, sent_tokenize

text = "Hello! How are you doing today? I'm learning NLP. It's really interesting."

# Word tokenization
word_tokens = word_tokenize(text)
print("Word tokens:", word_tokens)

# Sentence tokenization
sent_tokens = sent_tokenize(text)
print("\nSentence tokens:")
for i, sent in enumerate(sent_tokens):
    print(f"  {i+1}. {sent}")

# NLTK handles contractions and punctuation better
text = "I can't believe it's not butter! Don't you think so?"

print("Basic split:", text.split())
print("NLTK tokenize:", word_tokenize(text))

# TODO: Exercise 1.1
# Tokenize the following text into words using NLTK
# Count how many tokens are produced

import nltk
nltk.download('punkt')  # ensure tokenizer models are available
from nltk.tokenize import word_tokenize

text = "Dr. Smith's patients can't understand why they're feeling unwell. Is it the flu?"

# Tokenize
tokens = word_tokenize(text)

# Count tokens
num_tokens = len(tokens)

print("Tokens:", tokens)
print("Number of tokens:", num_tokens)

assert num_tokens == 18, f"Expected 18 tokens, got {num_tokens}"

# spaCy tokenization
text = "Apple is looking at buying U.K. startup for $1 billion."

doc = nlp(text)

# Get tokens
tokens = [token.text for token in doc]
print("Tokens:", tokens)

# spaCy provides additional information
print("\nDetailed token info:")
for token in doc:
    print(f"  {token.text:12} | POS: {token.pos_:6} | Is Stop: {token.is_stop}")

# TODO: Exercise 1.2
# Use spaCy to tokenize the text and extract only:
# 1. Tokens that are NOT punctuation
# 2. Tokens that are NOT spaces
# Hint: use token.is_punct and token.is_space

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

text = "The quick, brown fox jumps over the lazy dog! Isn't it amazing?"

doc = nlp(text)

# Filter tokens
clean_tokens = [token.text for token in doc if not token.is_punct and not token.is_space]

print("Clean tokens:", clean_tokens)
assert len(clean_tokens) == 13, f"Expected 13 tokens, got {len(clean_tokens)}"

from nltk.tokenize import RegexpTokenizer, TweetTokenizer

# RegexpTokenizer - tokenize using a custom pattern
# \w+ matches word characters only (removes punctuation)
regexp_tokenizer = RegexpTokenizer(r'\w+')

text = "Hello! How's it going? #NLP @user123"
print("Regexp tokens:", regexp_tokenizer.tokenize(text))

# TweetTokenizer - designed for social media text
tweet_tokenizer = TweetTokenizer()
print("Tweet tokens:", tweet_tokenizer.tokenize(text))

# TODO: Exercise 1.3
# Create a RegexpTokenizer that extracts only alphabetic words (no numbers, no punctuation)
# Pattern hint: [a-zA-Z]+ matches one or more letters

from nltk.tokenize import RegexpTokenizer

text = "I have 3 cats and 2 dogs! Their names are Max123 and Bella."

# Create tokenizer
alpha_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')

# Tokenize text
alpha_tokens = alpha_tokenizer.tokenize(text)

print("Alphabetic tokens:", alpha_tokens)
assert alpha_tokens == ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella'], "Check your pattern!"

from nltk.stem import PorterStemmer, SnowballStemmer

# Porter Stemmer (most common)
porter = PorterStemmer()

words = ["running", "runs", "ran", "runner", "easily", "fairly"]

print("Porter Stemmer:")
for word in words:
    print(f"  {word:12} -> {porter.stem(word)}")

# Snowball Stemmer (supports multiple languages)
snowball = SnowballStemmer('english')

print("Snowball Stemmer:")
for word in words:
    print(f"  {word:12} -> {snowball.stem(word)}")

# Available languages
print("\nAvailable languages:", SnowballStemmer.languages)

# Stemming limitations - can produce non-words
problem_words = ["studies", "studying", "university", "universe", "beautiful", "beauty"]

print("Stemming can produce non-words:")
for word in problem_words:
    print(f"  {word:12} -> {porter.stem(word)}")

# TODO: Exercise 2.1
# Apply Porter stemming to all words in the sentence
# Return the stemmed tokens as a list

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

nltk.download('punkt')  # ensure tokenizer models are available

sentence = "The cats are running and jumping over the sleeping dogs"

# Step 1: Tokenize
tokens = word_tokenize(sentence)

# Step 2: Apply stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(token.lower()) for token in tokens]

print("Original:", tokens)
print("Stemmed:", stemmed_tokens)

assert stemmed_tokens == ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog'], "Check your stemming!"

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

words = ["running", "runs", "ran", "better", "studies", "feet", "geese"]

print("Lemmatization (default - assumes nouns):")
for word in words:
    print(f"  {word:12} -> {lemmatizer.lemmatize(word)}")

# Lemmatization works better with POS tags
# pos: 'n' = noun, 'v' = verb, 'a' = adjective, 'r' = adverb

print("Lemmatization with POS tags:")
print(f"  running (verb):     {lemmatizer.lemmatize('running', pos='v')}")
print(f"  running (noun):     {lemmatizer.lemmatize('running', pos='n')}")
print(f"  better (adjective): {lemmatizer.lemmatize('better', pos='a')}")
print(f"  studies (verb):     {lemmatizer.lemmatize('studies', pos='v')}")
print(f"  studies (noun):     {lemmatizer.lemmatize('studies', pos='n')}")

# TODO: Exercise 2.2
# Lemmatize the following words using the correct POS tag

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Format: (word, pos_tag)
words_with_pos = [
    ("flying", "v"),      # verb -> fly
    ("happily", "r"),     # adverb -> happily
    ("worse", "a"),       # adjective -> bad
    ("mice", "n"),        # noun -> mouse
    ("are", "v"),         # verb -> be
]

print("Lemmatization results:")
for word, pos in words_with_pos:
    lemma = lemmatizer.lemmatize(word, pos=pos)
    print(f"  {word:12} ({pos}) -> {lemma}")

# Verify your answers
expected = ['fly', 'happily', 'bad', 'mouse', 'be']
results = [lemmatizer.lemmatize(w, pos=p) for w, p in words_with_pos]
assert results == expected, f"Expected {expected}, got {results}"

# spaCy automatically determines POS and lemmatizes correctly
text = "The children are playing with their toys. They were running and jumping happily."

doc = nlp(text)

print("spaCy lemmatization (automatic POS detection):")
for token in doc:
    if token.text != token.lemma_:  # Only show words that change
        print(f"  {token.text:12} ({token.pos_:5}) -> {token.lemma_}")

# TODO: Exercise 2.3
# Use spaCy to extract the lemmas of all non-punctuation tokens
# Return as a list of lowercase lemmas

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

text = "The dogs were barking loudly at the cats who were climbing the trees."

doc = nlp(text)

# Extract lemmas of non-punctuation tokens
lemmas = [token.lemma_.lower() for token in doc if not token.is_punct]

print("Lemmas:", lemmas)
assert lemmas == ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree'], "Check your lemmatization!"



# Compare the two approaches
words = ["studies", "studying", "better", "feet", "ran", "easily", "fairly", "wolves"]

porter = PorterStemmer()
lemmatizer = WordNetLemmatizer()

print(f"{'Word':<12} {'Stemmed':<12} {'Lemmatized':<12}")
print("-" * 36)
for word in words:
    stemmed = porter.stem(word)
    # For comparison, we'll use noun as default
    lemmatized = lemmatizer.lemmatize(word)
    print(f"{word:<12} {stemmed:<12} {lemmatized:<12}")

from nltk.corpus import stopwords

# Get English stop words
stop_words_nltk = set(stopwords.words('english'))

print(f"Number of NLTK stop words: {len(stop_words_nltk)}")
print(f"\nSample stop words: {list(stop_words_nltk)[:20]}")

# Remove stop words from text
text = "The quick brown fox jumps over the lazy dog in the park"
tokens = word_tokenize(text.lower())

# Filter out stop words
filtered_tokens = [token for token in tokens if token not in stop_words_nltk]

print("Original tokens:", tokens)
print("Without stop words:", filtered_tokens)

# TODO: Exercise 3.1
# Remove stop words from the following text and return the remaining tokens
# Make sure to lowercase the text first!

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

text = "This is a sample sentence showing the removal of stop words from the text"

# Step 1: Lowercase and tokenize
tokens = word_tokenize(text.lower())

# Step 2: Remove stop words
stop_words = set(stopwords.words('english'))
filtered = [token for token in tokens if token not in stop_words]

print("Filtered tokens:", filtered)
assert filtered == ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text'], "Check your filtering!"

# spaCy has built-in stop word detection
text = "This is a sample sentence for demonstrating stop word removal."

doc = nlp(text)

print("Token analysis:")
for token in doc:
    print(f"  {token.text:<15} is_stop: {token.is_stop}")

# Filter using spaCy's is_stop attribute
content_words = [token.text for token in doc if not token.is_stop and not token.is_punct]
print("Content words:", content_words)

# Customize stop words in spaCy
# Add custom stop words
nlp.vocab["sample"].is_stop = True

# Remove words from stop list
nlp.vocab["not"].is_stop = False  # 'not' carries meaning!

# Check the spaCy stop words list
print(f"Number of spaCy stop words: {len(nlp.Defaults.stop_words)}")

import string

# Python's string.punctuation
print("Punctuation characters:", string.punctuation)

# Method 1: Using str.translate()
text = "Hello, World! How's it going? #NLP @user"
translator = str.maketrans('', '', string.punctuation)
clean_text = text.translate(translator)
print("\nUsing translate():", clean_text)

# Method 2: Using regex
import re

text = "Hello, World! How's it going? #NLP @user 123"

# Remove all non-alphanumeric characters (keep spaces)
clean_text = re.sub(r'[^a-zA-Z\s]', '', text)
print("Regex (letters only):", clean_text)

# Remove punctuation but keep numbers
clean_text2 = re.sub(r'[^\w\s]', '', text)
print("Regex (alphanumeric):", clean_text2)

# Method 3: Using spaCy token attributes
text = "Hello! This is @user's tweet about #NLP. Check https://example.com!"

doc = nlp(text)

# Filter tokens
clean_tokens = [
    token.text for token in doc
    if not token.is_punct
    and not token.is_space
    and not token.like_url
]

print("Clean tokens:", clean_tokens)

# TODO: Exercise 3.2
# Clean the following text by:
# 1. Removing URLs
# 2. Removing mentions (@user)
# 3. Removing hashtags (#topic)
# 4. Removing punctuation
# 5. Converting to lowercase
# Use regex for this exercise

import re

text = "Check out @OpenAI's new model! https://openai.com #AI #MachineLearning It's amazing!!!"

clean = text

# Remove URLs
clean = re.sub(r'http[s]?://\S+', '', clean)

# Remove mentions
clean = re.sub(r'@\w+', '', clean)

# Remove hashtags
clean = re.sub(r'#\w+', '', clean)

# Remove punctuation
clean = re.sub(r"[^\w\s]", '', clean)

# Convert to lowercase
clean = clean.lower()

# Remove extra whitespace
clean = " ".join(clean.split())

print(f"Clean text: '{clean}'")
assert clean == "check out new model its amazing", f"Got: '{clean}'"

import re

text = "Check out @OpenAI's new model! https://openai.com #AI #MachineLearning It's amazing!!!"

clean = text

# Remove URLs
clean = re.sub(r'http[s]?://\S+', '', clean)

# Remove mentions along with trailing apostrophe + s (if present)
clean = re.sub(r'@\w+\'?s?', '', clean)

# Remove hashtags
clean = re.sub(r'#\w+', '', clean)

# Remove punctuation (leftover)
clean = re.sub(r"[^\w\s]", '', clean)

# Convert to lowercase
clean = clean.lower()

# Remove extra whitespace
clean = " ".join(clean.split())

print(f"Clean text: '{clean}'")
assert clean == "check out new model its amazing", f"Got: '{clean}'"

def preprocess_pipeline_example(text):
    """
    Example preprocessing pipeline.

    Steps:
    1. Lowercase
    2. Remove URLs
    3. Remove special characters
    4. Tokenize
    5. Remove stop words
    6. Lemmatize
    """
    # Step 1: Lowercase
    text = text.lower()

    # Step 2: Remove URLs
    text = re.sub(r'https?://\S+', '', text)

    # Step 3: Remove special characters (keep only letters and spaces)
    text = re.sub(r'[^a-z\s]', '', text)

    # Step 4: Tokenize with spaCy
    doc = nlp(text)

    # Step 5 & 6: Remove stop words and lemmatize
    tokens = [
        token.lemma_
        for token in doc
        if not token.is_stop
        and not token.is_punct
        and not token.is_space
        and len(token.text) > 1  # Remove single characters
    ]

    return tokens

# Test the pipeline
sample_text = """
The quick brown foxes are jumping over the lazy dogs!
Check out https://example.com for more information.
This is SO amazing!!! #NLP #Python @user123
"""

result = preprocess_pipeline_example(sample_text)
print("Preprocessed tokens:", result)

import re
import spacy
from nltk.corpus import stopwords
import nltk

# Download stopwords if needed
nltk.download('stopwords')

# Load spaCy model
nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    """
    Complete text preprocessing pipeline.

    Args:
        text (str): Raw input text

    Returns:
        list: List of preprocessed tokens
    """
    # Step 1: Lowercase
    text = text.lower()

    # Step 2: Remove URLs
    text = re.sub(r'http[s]?://\S+', '', text)

    # Step 3: Remove emails
    text = re.sub(r'\S+@\S+', '', text)

    # Step 4: Remove mentions and hashtags
    text = re.sub(r'[@#]\w+', '', text)

    # Step 5: Remove numbers
    text = re.sub(r'\d+', '', text)

    # Step 6: Remove punctuation/special characters
    text = re.sub(r'[^\w\s]', '', text)

    # Step 7: Tokenize with spaCy
    doc = nlp(text)

    # Step 8 & 9: Remove stop words and lemmatize
    tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_space]

    # Step 10: Remove tokens with less than 2 characters
    tokens = [token for token in tokens if len(token) > 1]

    return tokens

# Test the pipeline
raw_text = """
    ðŸš€ Check out our NEW product at https://example.com!!!
    Contact: sales@company.com @CompanyName #Innovation #Tech
    Limited time offer: 50% OFF!!!
"""

cleaned_tokens = preprocess_text(raw_text)
print("Cleaned tokens:", cleaned_tokens)

# Test your pipeline with this text
test_text = """
ðŸš€ BREAKING NEWS!!! The researchers at @MIT have published 5 new papers
about Natural Language Processing! Check out https://mit.edu/nlp for details.
Contact them at research@mit.edu for collaborations. #NLP #AI #Research

The experiments were conducted using state-of-the-art transformers models.
They achieved 95.5% accuracy on the benchmark datasets!!!
"""

result = preprocess_text(test_text)
print("Preprocessed tokens:")
print(result)

# Verify some expected tokens are in the result
expected_tokens = ['researcher', 'publish', 'paper', 'natural', 'language', 'processing']
for token in expected_tokens:
    assert token in result, f"Expected '{token}' in result"

# Verify unwanted elements are NOT in result
unwanted = ['@mit', 'https', 'mit.edu', '#nlp', '95.5', '!!!', 'the', 'a', 'at']
for item in unwanted:
    assert item not in result, f"'{item}' should not be in result"

print("\nâœ… All tests passed!")

# TODO: Exercise 4.2
# Apply your preprocessing pipeline to a list of documents
# Return a list of lists (one list of tokens per document)

documents = [
    "Machine learning is transforming the tech industry! @Google #ML",
    "I love programming in Python. It's so easy to learn! https://python.org",
    "The cats are sleeping on the couch. They're so lazy!",
    "Contact support@company.com for any questions about our AI products."
]

# Apply the preprocessing pipeline to each document
processed_docs = [preprocess_text(doc) for doc in documents]

print("Processed documents:")
for i, doc in enumerate(processed_docs):
    print(f"  Doc {i+1}: {doc}")